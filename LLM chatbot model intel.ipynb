{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0473c6-3734-422d-a370-2e39d576be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q \"torch>=2.1\" \"nncf>=2.7\" \"transformers>=4.36.0\" onnx \"optimum>=1.16.1\" \"accelerate\" \"datasets>=2.14.6\" \"gradio>=4.19\" \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51920510-effc-49ef-8c6f-81c951d96a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "from notebook_utils import download_file\n",
    "\n",
    "if not Path(\"./config.py\").exists():\n",
    "    download_file(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/llm-question-answering/config.py\")\n",
    "from config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b42290-a9b5-4453-9a4c-ffa44bbd966d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70b174632e5443988b7e1dc3ba107a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=1, options=('tiny-llama-1b', 'phi-2', 'dolly-v2-3b', 'red-pajama-instructâ€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids,\n",
    "    value=model_ids[1],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e9634f-4fc7-4d9c-9ade-b3e8684a0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model phi-2\n"
     ]
    }
   ],
   "source": [
    "model_configuration = SUPPORTED_LLM_MODELS[model_id.value]\n",
    "print(f\"Selected model {model_id.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7937959a-d2a1-49bd-bf12-35554fa901d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'susnato/phi-2',\n",
       " 'prompt_template': 'Instruct:{instruction}\\nOutput:'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configuration = SUPPORTED_LLM_MODELS[model_id.value]\n",
    "model_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81602ca-4674-4b61-b2c8-ca11631428b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c488e44677034e7685a24c07a07d2fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea4fdd27f564bd2bdd44d6503b730e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57375b95dca4e749ad4b882facd5a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0066fbec-b89b-4caa-94fb-9ea8598c22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import openvino as ov\n",
    "import nncf\n",
    "from optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\n",
    "import gc\n",
    "\n",
    "\n",
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "pt_model_id = model_configuration[\"model_id\"]\n",
    "fp16_model_dir = Path(model_id.value) / \"FP16\"\n",
    "int8_model_dir = Path(model_id.value) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(model_id.value) / \"INT4_compressed_weights\"\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(pt_model_id, export=True, compile=False, load_in_8bit=False)\n",
    "    ov_model.half()\n",
    "    ov_model.save_pretrained(fp16_model_dir)\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(pt_model_id, export=True, compile=False, load_in_8bit=True)\n",
    "    ov_model.save_pretrained(int8_model_dir)\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"mistral-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"red-pajama-3b-instruct\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.5,\n",
    "        },\n",
    "        \"dolly-v2-3b\": {\"sym\": False, \"group_size\": 32, \"ratio\": 0.5},\n",
    "        \"llama-3-8b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"default\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    model_compression_params = compression_configs.get(model_id.value, compression_configs[\"default\"])\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(\n",
    "        pt_model_id,\n",
    "        export=True,\n",
    "        compile=False,\n",
    "        quantization_config=OVWeightQuantizationConfig(bits=4, **model_compression_params),\n",
    "    )\n",
    "    ov_model.save_pretrained(int4_model_dir)\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c7b254-1ce4-4f23-813a-9bdc23aed327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model with INT8 compressed weights is 2656.55 MB\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d7bf5b-8a05-4c3b-a36b-631af5c197e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f8b67dab854370b54d2615a94dbac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "if \"NPU\" in support_devices:\n",
    "    support_devices.remove(\"NPU\")\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01673d97-6645-4d2a-8306-293b8064b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24532480-80a5-4953-9cd6-78ac51a1cd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd64773489446ad80d0cd2923a8d9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT8',), value='INT8')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5259c1c5-4128-4210-9ad2-faf33ee40e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from phi-2\\INT8_compressed_weights\n",
      "susnato/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "model_name = model_configuration[\"model_id\"]\n",
    "print(model_name)\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device.value,\n",
    "    ov_config=ov_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d23a99-3284-42db-b7dc-5805d219f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import perf_counter\n",
    "from typing import List\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2638c5b-47ad-4213-80da-8cfc2659b3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_kwargs = model_configuration.get(\"toeknizer_kwargs\", {})\n",
    "\n",
    "\n",
    "def get_special_token_id(tokenizer: AutoTokenizer, key: str) -> int:\n",
    "    \"\"\"\n",
    "    Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "\n",
    "response_key = model_configuration.get(\"response_key\")\n",
    "tokenizer_response_key = None\n",
    "\n",
    "if response_key is not None:\n",
    "    tokenizer_response_key = next(\n",
    "        (token for token in tokenizer.additional_special_tokens if token.startswith(response_key)),\n",
    "        None,\n",
    "    )\n",
    "\n",
    "end_key_token_id = None\n",
    "if tokenizer_response_key:\n",
    "    try:\n",
    "        end_key = model_configuration.get(\"end_key\")\n",
    "        if end_key:\n",
    "            end_key_token_id = get_special_token_id(tokenizer, end_key)\n",
    "        # Ensure generation stops once it generates \"### End\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "prompt_template = model_configuration.get(\"prompt_template\", \"{instruction}\")\n",
    "end_key_token_id = end_key_token_id or tokenizer.eos_token_id\n",
    "pad_token_id = end_key_token_id or tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27802e81-9d42-4d71-99ea-5f76db5237f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(\n",
    "    user_text: str,\n",
    "    top_p: float,\n",
    "    temperature: float,\n",
    "    top_k: int,\n",
    "    max_new_tokens: int,\n",
    "    perf_text: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Text generation function\n",
    "\n",
    "    Parameters:\n",
    "      user_text (str): User-provided instruction for a generation.\n",
    "      top_p (float):  Nucleus sampling. If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for a generation.\n",
    "      temperature (float): The value used to module the logits distribution.\n",
    "      top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "      max_new_tokens (int): Maximum length of generated sequence.\n",
    "      perf_text (str): Content of text field for printing performance results.\n",
    "    Returns:\n",
    "      model_output (str) - model-generated text\n",
    "      perf_text (str) - updated perf text filed content\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare input prompt according to model expected template\n",
    "    prompt_text = prompt_template.format(instruction=user_text)\n",
    "\n",
    "    # Tokenize the user text.\n",
    "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
    "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=float(temperature),\n",
    "        top_k=top_k,\n",
    "        eos_token_id=end_key_token_id,\n",
    "        pad_token_id=pad_token_id,\n",
    "    )\n",
    "    t = Thread(target=ov_model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    # Pull the generated text from the streamer, and update the model output.\n",
    "    model_output = \"\"\n",
    "    per_token_time = []\n",
    "    num_tokens = 0\n",
    "    start = perf_counter()\n",
    "    for new_text in streamer:\n",
    "        current_time = perf_counter() - start\n",
    "        model_output += new_text\n",
    "        perf_text, num_tokens = estimate_latency(current_time, perf_text, new_text, per_token_time, num_tokens)\n",
    "        yield model_output, perf_text\n",
    "        start = perf_counter()\n",
    "    return model_output, perf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b2a2b7-5a91-470f-98d7-7355664bc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_latency(\n",
    "    current_time: float,\n",
    "    current_perf_text: str,\n",
    "    new_gen_text: str,\n",
    "    per_token_time: List[float],\n",
    "    num_tokens: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function for performance estimation\n",
    "\n",
    "    Parameters:\n",
    "      current_time (float): This step time in seconds.\n",
    "      current_perf_text (str): Current content of performance UI field.\n",
    "      new_gen_text (str): New generated text.\n",
    "      per_token_time (List[float]): history of performance from previous steps.\n",
    "      num_tokens (int): Total number of generated tokens.\n",
    "\n",
    "    Returns:\n",
    "      update for performance text field\n",
    "      update for a total number of tokens\n",
    "    \"\"\"\n",
    "    num_current_toks = len(tokenizer.encode(new_gen_text))\n",
    "    num_tokens += num_current_toks\n",
    "    per_token_time.append(num_current_toks / current_time)\n",
    "    if len(per_token_time) > 10 and len(per_token_time) % 4 == 0:\n",
    "        current_bucket = per_token_time[:-10]\n",
    "        return (\n",
    "            f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}\",\n",
    "            num_tokens,\n",
    "        )\n",
    "    return current_perf_text, num_tokens\n",
    "\n",
    "\n",
    "def reset_textbox(instruction: str, response: str, perf: str):\n",
    "    \"\"\"\n",
    "    Helper function for resetting content of all text fields\n",
    "\n",
    "    Parameters:\n",
    "      instruction (str): Content of user instruction field.\n",
    "      response (str): Content of model response field.\n",
    "      perf (str): Content of performance info filed\n",
    "\n",
    "    Returns:\n",
    "      empty string for each placeholder\n",
    "    \"\"\"\n",
    "    return \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f222d02-847a-490f-8d66-02608a53259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Give me a recipe for pizza with pineapple\",\n",
    "    \"Write me a tweet about the new OpenVINO release\",\n",
    "    \"Explain the difference between CPU and GPU\",\n",
    "    \"Give five ideas for a great weekend with family\",\n",
    "    \"Do Androids dream of Electric sheep?\",\n",
    "    \"Who is Dolly?\",\n",
    "    \"Please give me advice on how to write resume?\",\n",
    "    \"Name 3 advantages to being a cat\",\n",
    "    \"Write instructions on how to become a good AI engineer\",\n",
    "    \"Write a love letter to my best friend\",\n",
    "]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"# Question Answering with \" + model_id.value + \" and OpenVINO.\\n\"\n",
    "        \"Provide instruction which describes a task below or select among predefined examples and model writes response that performs requested task.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            user_text = gr.Textbox(\n",
    "                placeholder=\"Write an email about an alpaca that likes flan\",\n",
    "                label=\"User instruction\",\n",
    "            )\n",
    "            model_output = gr.Textbox(label=\"Model response\", interactive=False)\n",
    "            performance = gr.Textbox(label=\"Performance\", lines=1, interactive=False)\n",
    "            with gr.Column(scale=1):\n",
    "                button_clear = gr.Button(value=\"Clear\")\n",
    "                button_submit = gr.Button(value=\"Submit\")\n",
    "            gr.Examples(examples, user_text)\n",
    "        with gr.Column(scale=1):\n",
    "            max_new_tokens = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=1000,\n",
    "                value=256,\n",
    "                step=1,\n",
    "                interactive=True,\n",
    "                label=\"Max New Tokens\",\n",
    "            )\n",
    "            top_p = gr.Slider(\n",
    "                minimum=0.05,\n",
    "                maximum=1.0,\n",
    "                value=0.92,\n",
    "                step=0.05,\n",
    "                interactive=True,\n",
    "                label=\"Top-p (nucleus sampling)\",\n",
    "            )\n",
    "            top_k = gr.Slider(\n",
    "                minimum=0,\n",
    "                maximum=50,\n",
    "                value=0,\n",
    "                step=1,\n",
    "                interactive=True,\n",
    "                label=\"Top-k\",\n",
    "            )\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.1,\n",
    "                maximum=5.0,\n",
    "                value=0.8,\n",
    "                step=0.1,\n",
    "                interactive=True,\n",
    "                label=\"Temperature\",\n",
    "            )\n",
    "\n",
    "    user_text.submit(\n",
    "        run_generation,\n",
    "        [user_text, top_p, temperature, top_k, max_new_tokens, performance],\n",
    "        [model_output, performance],\n",
    "    )\n",
    "    button_submit.click(\n",
    "        run_generation,\n",
    "        [user_text, top_p, temperature, top_k, max_new_tokens, performance],\n",
    "        [model_output, performance],\n",
    "    )\n",
    "    button_clear.click(\n",
    "        reset_textbox,\n",
    "        [user_text, model_output, performance],\n",
    "        [user_text, model_output, performance],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue()\n",
    "    try:\n",
    "        demo.launch(height=800)\n",
    "    except Exception:\n",
    "        demo.launch(share=True, height=800)\n",
    "\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4a744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dcbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecfdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/daafd702-5a42-4f54-ae72-2e4480d73501",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
